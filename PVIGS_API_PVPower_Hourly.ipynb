{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da541e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://joint-research-centre.ec.europa.eu/pvgis-online-tool/getting-started-pvgis/api-non-interactive-service_en\n",
    "\n",
    "import os, csv, json, requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, date\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from pathlib import Path\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe37c5-cadb-40dd-8261-a6207cbdb4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__  = \"Fernando Fahl <fernando.fahl@gmail.com>\"\n",
    "__version__ = \"1.1\"\n",
    "__date__    = \"March 2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023ff72-c38d-445b-8a50-179b7d35828f",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c24d73d-1369-4bed-b0d4-ae9c8ae5171b",
   "metadata": {},
   "source": [
    "## 1.1 Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ebe686-3102-413f-ba7e-19efa35be738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ____________ API parameters\n",
    "startyear     = 2020\n",
    "endyear       = 2020\n",
    "peakpower     = 1\n",
    "loss          = 14\n",
    "pvcalculation = 1 # \"0\" outputs only solar radiation calculations, \"1\" outputs the estimation of hourly PV production as well\n",
    "optimalangles = 1 #  Value of 1 for \"yes\". All other values (or no value) mean \"no\". \n",
    "\n",
    "# ____________ location points (from excel file): required fields\n",
    "excel_filename = 'EU_random_points.xlsx'\n",
    "column_id      = 'id'\n",
    "column_lat     = 'latitude'\n",
    "column_lon     = 'longitude'\n",
    "column_timezone = 'time_zone' # time zone must be one of the columns. In case of no time zone transformation, use 0\n",
    "\n",
    "# ____________ compression output file (True=yes, False=No)\n",
    "compress_output = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5727d7-9d1c-44af-96bf-3464f6146891",
   "metadata": {},
   "source": [
    "# 2. Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fead04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "today       = date.today().isoformat()\n",
    "home        = Path(os.getcwd())\n",
    "xls_file    = home.joinpath(excel_filename)\n",
    "\n",
    "csv_outfile = home.joinpath(f'{xls_file.stem}_year{startyear}to{endyear}_{today}.csv')\n",
    "csv_errors  = home.joinpath(f'{xls_file.stem}_ERRORs_{today}.csv')\n",
    "\n",
    "print (csv_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d6b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xls = pd.read_excel(xls_file)\n",
    "columns = [column_id,column_lat,column_lon,column_timezone]\n",
    "df = df_xls[columns].copy()\n",
    "df.set_index(column_id, inplace=True)\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df0ab8-43d2-4631-bc8e-5b1b804381a5",
   "metadata": {},
   "source": [
    "# 3. PVGIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a58f4f0-3957-41bf-856c-aa89a4cff2dd",
   "metadata": {},
   "source": [
    "## 3.1 Create URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f351a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(lat, lon):    \n",
    "    \n",
    "    url_base = \"https://re.jrc.ec.europa.eu/api/v5_2/seriescalc?\"\n",
    "    \n",
    "    pvgis_params = dict(\n",
    "        peakpower=peakpower,\n",
    "        loss=loss,\n",
    "        startyear=startyear,\n",
    "        endyear=endyear,\n",
    "        pvcalculation=pvcalculation, # \"0\" outputs only solar radiation calculations, \"1\" outputs the estimation of hourly PV production as well\n",
    "        optimalangles=optimalangles,\n",
    "        lat=lat,\n",
    "        lon=lon,\n",
    "        outputformat = 'json',\n",
    "    )   \n",
    "\n",
    "    return url_base + \"&\".join([f'{key}={value}' for key, value in pvgis_params.items()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c707ad5-6bd6-43cd-8dcc-c3b5398fb81b",
   "metadata": {},
   "source": [
    "## 3.2 Parse PVGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b52fad-0f64-4be5-a2f0-ae23b6fb87a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(id, data, time_zone, startyear, endyear):    \n",
    "    \n",
    "    # ____________ parse data to df\n",
    "    df_input        = pd.json_normalize(data.get('inputs'))\n",
    "    df_output       = pd.json_normalize(data.get('outputs').get('hourly'))        \n",
    "    latitude        = df_input['location.latitude'].values\n",
    "    \n",
    "    # ____________ get optimal angles\n",
    "    slope_optimized   = df_input['mounting_system.fixed.slope.value'].values\n",
    "    azimuth_optimized = df_input['mounting_system.fixed.azimuth.value'].values    \n",
    "    azimuth_corrected = 0 if float(latitude) < 0 else 180\n",
    "\n",
    "    # ____________ convert the 'Date' column to datetime format: round minutes from timestamp (do not trunc the timestamp - it would cause a shift in some readings). problems to round hours when it is .30 (generates duplicates) - solution below solves the problem\n",
    "    df_power               = df_output[['time', 'P']].copy() \n",
    "    df_power['time']       = pd.to_datetime(df_power['time'], format='%Y%m%d:%H%M')\n",
    "    df_power['nearest_hour'] = df_power[\"time\"].dt.ceil(\"H\").where(df_power[\"time\"].dt.minute > 30, df_power[\"time\"].dt.floor(\"H\"))\n",
    "    df_power['local_time'] = df_power['nearest_hour'] + timedelta(hours = time_zone)\n",
    "    \n",
    "    # ____________ reformat timestamp (remove minutes)\n",
    "    df_power['local_time']  = df_power['local_time'].dt.strftime('%Y-%m-%d:%H')    \n",
    "    \n",
    "    # ____________ select data for the data range (time shift generates data for the next year for positive zones and previous year for negative zones)   \n",
    "    date_start = f'{startyear}-01-01:00'\n",
    "    date_end   = f'{endyear}-12-31:23'    \n",
    "    df_mask    = df_power.loc[df_power['local_time'].between(date_start, date_end, inclusive='both')]\n",
    "    \n",
    "    # # ____________ fill missing data (it can give error if \n",
    "    # df_mask = df_mask.set_index(df_mask['local_time'])\n",
    "    # new_date_range = pd.date_range(start=datetime.strptime(date_start, '%Y-%m-%d:%H'), end=datetime.strptime(date_end, '%Y-%m-%d:%H'), freq=\"H\").strftime('%Y-%m-%d:%H')\n",
    "    # df_mask.reindex(new_date_range, fill_value=0)\n",
    "     \n",
    "    # ____________ set datetime index\n",
    "    df_mask = df_mask.set_index(df_mask['local_time'])\n",
    "    df_mask = df_mask.drop(['time', 'local_time'], axis=1)   \n",
    "    \n",
    "    # ____________ transpose df\n",
    "    df_tranpose = df_mask[['P']].transpose()   \n",
    "    \n",
    "    # ____________ add columns\n",
    "    df_tranpose.insert(0, 'time_zone', time_zone)\n",
    "    df_tranpose.insert(0, 'azimuth_corrected', azimuth_corrected)\n",
    "    df_tranpose.insert(0, 'azimuth', azimuth_optimized)\n",
    "    df_tranpose.insert(0, 'slope', slope_optimized)\n",
    "    df_tranpose.insert(0, 'id', id)   \n",
    "    \n",
    "    return df_tranpose    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87506f-4340-4b45-b0d7-e810da19b7de",
   "metadata": {},
   "source": [
    "## 3.3 Query PVGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76daeca6-783b-44c8-bc93-3b4a31665896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(dfs, dfs_error):\n",
    "    \"\"\" save csv as compressed files\"\"\"\n",
    "    \n",
    "    if compress_output:\n",
    "        compression='gzip'\n",
    "        out_name=f\"{csv_outfile}.gz\"\n",
    "        out_name_error=f\"{csv_errors}.gz\"\n",
    "    else:\n",
    "        compression=None\n",
    "        out_name=csv_outfile\n",
    "        out_name_error=csv_errors\n",
    "\n",
    "    df_merged = pd.concat(dfs.values(), axis=0, ignore_index=True)       \n",
    "    df_merged.to_csv(out_name, index = False, compression='gzip')\n",
    "    print (f\"\\n...saving file: {out_name}\")\n",
    "    \n",
    "    if dfs_error:    \n",
    "        df_merged_error = pd.concat(dfs_error.values(), axis=0, ignore_index=True)    \n",
    "        df_merged_error.to_csv(out_name_error, index = False, compression='gzip')  \n",
    "        print (f\"...saving file: {out_name_error}\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4135047b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "dfs = dict()\n",
    "dfs_error = dict()\n",
    "\n",
    "for idx, row in df.iterrows(): \n",
    "    \n",
    "    id   = idx\n",
    "    lat  = row[column_lat].astype(str)\n",
    "    lon  = row[column_lon].astype(str)\n",
    "    time_zone = int(row[column_timezone])\n",
    "    url  = get_url (lat,lon)\n",
    "\n",
    "    # _____________ get data from pvgis\n",
    "    response = requests.get(url)\n",
    "    row_json = json.loads(response.text)    \n",
    "\n",
    "    # _____________ parse data from pvgis\n",
    "    try:            \n",
    "        dfs[id] = parse_json(id, row_json, time_zone, startyear, endyear)\n",
    "        print (f'done {i}: id={id} time zone={time_zone} url={url}')\n",
    "    except:\n",
    "        message = row_json['message']\n",
    "        dfs_error[id] = pd.DataFrame(dict(id=id,lat=lat,lon=lon,message=message,url=url), index=[0])\n",
    "        print (f'ERROR {i} --> id: {id}, message: {message} url: {url}')\n",
    "\n",
    "    # _____________ save partial results (if it contains more than 500 points)\n",
    "    if i > 500:  \n",
    "        save_csv(dfs, dfs_error)        \n",
    "        i = 0\n",
    "        clear_output(wait=False)  \n",
    "    i +=1      \n",
    "            \n",
    "# _____________ save final results\n",
    "save_csv(dfs, dfs_error)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45ded8-f3b1-47fe-a299-9470619ee541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
